{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random as jrng\n",
    "from jax import numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_string():\n",
    "    return ''.join([random.choice(string.ascii_letters + string.digits) for n in range(32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "def split_and_sample(key, shape):\n",
    "    key, subkey = jrng.split(key)\n",
    "    val = jrng.normal(subkey, shape=shape)\n",
    "    return key, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, name=None):\n",
    "        if name is None:\n",
    "            self.name = F'Layer+{rand_string()}'\n",
    "        else:\n",
    "            self.name = name\n",
    "    \n",
    "    def __call__(self, p, x):\n",
    "        return self.forward(p, x)\n",
    "        \n",
    "    def params(self):\n",
    "        return None\n",
    "    \n",
    "    def init_params(self, rng):\n",
    "        return rng, self.params()\n",
    "    \n",
    "    def forward(self, p, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, d_in, d_out, name=None):\n",
    "        super(Linear, self).__init__(name)\n",
    "        \n",
    "        self.weight = jnp.zeros((d_in, d_out))\n",
    "        self.bias = jnp.zeros((d_out))\n",
    "        \n",
    "        if name is None:\n",
    "            self.name = F'Linear+{rand_string()}'\n",
    "        \n",
    "    def params(self):\n",
    "        return dict([('weight', self.weight), ('bias', self.bias)])\n",
    "    \n",
    "    def init_params(self, rng):\n",
    "        rng, self.weight = split_and_sample(rng, self.weight.shape)\n",
    "        return rng, self.params()\n",
    "    \n",
    "    def forward(self, p, x):\n",
    "        return jnp.dot(x, p['weight']) + p['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(Tanh, self).__init__(name)\n",
    "        \n",
    "        if name is None:\n",
    "            self.name = F'Tanh+{rand_string()}'\n",
    "            \n",
    "    def forward(self, p, x):\n",
    "        return jnp.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(Softmax, self).__init__(name)\n",
    "        \n",
    "        if name is None:\n",
    "            self.name = F'Softmax+{rand_string()}'\n",
    "            \n",
    "    def forward(self, p, x):\n",
    "        x_exp = jnp.exp(x)\n",
    "        return x_exp / jnp.sum(x_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, rng, layers, loss=None, name=None):        \n",
    "        if name is None:\n",
    "            name = F'Model+{rand_string()}'\n",
    "            \n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "            \n",
    "        self.params = dict()\n",
    "        for ll in self.layers:\n",
    "            rng, pp = ll.init_params(rng)\n",
    "            if pp is not None:\n",
    "                self.params[ll.name] = pp\n",
    "        self.params_values, self.params_tree = jax.tree_flatten(self.params)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def forward_(self, p, x):\n",
    "        h = x\n",
    "        for ll in self.layers:\n",
    "            h = ll(None if ll.name not in p else p[ll.name], h)\n",
    "        return h    \n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def loss_(self, p, x, y):\n",
    "        def dummy(mymodel, params, x, y):\n",
    "            return mymodel.loss(x, y)\n",
    "        return jax.vmap(dummy, in_axes=(None,None,0,0))(self, self.params, self.forward_(p, x), y).mean()\n",
    "    \n",
    "    def forward(self, x, single=False):\n",
    "        if single:\n",
    "            return self.forward_(self.params, x)\n",
    "        \n",
    "        def dummy(mymodel, params, x):\n",
    "            return mymodel.forward_(params, x)\n",
    "        return jax.vmap(dummy, in_axes=(None, None, 0))(self, self.params, x)\n",
    "    \n",
    "    def loss_grad(self, x, y):\n",
    "        return self.loss_(self.params, x, y), jax.grad(self.loss_)(self.params, x, y)\n",
    "    \n",
    "    ''' TODO: implement a slightly fancier optimizer '''\n",
    "    def step(self, grad, lr=.001):\n",
    "        for ll in self.layers:\n",
    "            if ll.name not in self.params:\n",
    "                continue\n",
    "            pp = self.params[ll.name]\n",
    "            gg = grad[ll.name]\n",
    "            for kk in pp.keys():\n",
    "                pp[kk] = pp[kk] - lr * gg[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def cross_entropy(p, y):\n",
    "    return -jnp.take(jnp.log(p), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jrng.PRNGKey(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = Model(rng, [Linear(10,10), Tanh(), Linear(10,10), Softmax()], loss=cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.155117\n",
      "6.069771\n",
      "6.008048\n",
      "5.9613843\n",
      "5.9248037\n",
      "5.8952703\n",
      "5.8708405\n",
      "5.8502226\n",
      "5.832527\n",
      "5.817123\n"
     ]
    }
   ],
   "source": [
    "target_labels = numpy.floor(numpy.random.rand(256)).astype('int')\n",
    "inputs = numpy.random.rand(256,10)\n",
    "\n",
    "for ii in range(1000):\n",
    "    loss, grad = mymodel.loss_grad(inputs, target_labels)\n",
    "    \n",
    "    if numpy.mod(ii, 100) == 0:\n",
    "        print(loss)\n",
    "    mymodel.step(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
