{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random as jrng\n",
    "from jax import numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_string():\n",
    "    return ''.join([random.choice(string.ascii_letters + string.digits) for n in range(32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "def split_and_sample(key, shape):\n",
    "    key, subkey = jrng.split(key)\n",
    "    val = jrng.normal(subkey, shape=shape)\n",
    "    return key, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, name=None):\n",
    "        if name is None:\n",
    "            self.name = F'Layer+{rand_string()}'\n",
    "        else:\n",
    "            self.name = name\n",
    "    \n",
    "    def __call__(self, p, x):\n",
    "        return self.forward(p, x)\n",
    "        \n",
    "    def params(self):\n",
    "        return None\n",
    "    \n",
    "    def init_params(self, rng):\n",
    "        return rng, self.params()\n",
    "    \n",
    "    def forward(self, p, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, d_in, d_out, name=None):\n",
    "        super(Linear, self).__init__(name)\n",
    "        \n",
    "        self.weight = jnp.zeros((d_in, d_out))\n",
    "        self.bias = jnp.zeros((d_out))\n",
    "        \n",
    "        if name is None:\n",
    "            self.name = F'Linear+{rand_string()}'\n",
    "        \n",
    "    def params(self):\n",
    "        return dict([('weight', self.weight), ('bias', self.bias)])\n",
    "    \n",
    "    def init_params(self, rng):\n",
    "        rng, self.weight = split_and_sample(rng, self.weight.shape)\n",
    "        return rng, self.params()\n",
    "    \n",
    "    def forward(self, p, x):\n",
    "        return jnp.dot(x, p['weight']) + p['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(Tanh, self).__init__(name)\n",
    "        \n",
    "        if name is None:\n",
    "            self.name = F'Tanh+{rand_string()}'\n",
    "            \n",
    "    def forward(self, p, x):\n",
    "        return jnp.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(Softmax, self).__init__(name)\n",
    "        \n",
    "        if name is None:\n",
    "            self.name = F'Softmax+{rand_string()}'\n",
    "            \n",
    "    def forward(self, p, x):\n",
    "        x_exp = jnp.exp(x)\n",
    "        return x_exp / jnp.sum(x_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, rng, layers, loss=None, name=None):        \n",
    "        if name is None:\n",
    "            name = F'Model+{rand_string()}'\n",
    "            \n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "            \n",
    "        self.params = dict()\n",
    "        for ll in self.layers:\n",
    "            rng, pp = ll.init_params(rng)\n",
    "            if pp is not None:\n",
    "                self.params[ll.name] = pp\n",
    "        self.params_values, self.params_tree = jax.tree_flatten(self.params)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def forward_(self, p, x):\n",
    "        h = x\n",
    "        for ll in self.layers:\n",
    "            h = ll(None if ll.name not in p else p[ll.name], h)\n",
    "        return h    \n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def loss_(self, p, x, y):\n",
    "        def dummy(mymodel, params, x, y):\n",
    "            return mymodel.loss(x, y)\n",
    "        return jax.vmap(dummy, in_axes=(None,None,0,0))(self, self.params, self.forward_(p, x), y).mean()\n",
    "    \n",
    "    def forward(self, x, single=False):\n",
    "        if single:\n",
    "            return self.forward_(self.params, x)\n",
    "        \n",
    "        def dummy(mymodel, params, x):\n",
    "            return mymodel.forward_(params, x)\n",
    "        return jax.vmap(dummy, in_axes=(None, None, 0))(self, self.params, x)\n",
    "    \n",
    "    def loss_grad(self, x, y):\n",
    "        return self.loss_(self.params, x, y), jax.grad(self.loss_)(self.params, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, model, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        \n",
    "    def step(self, grad):\n",
    "        for ll in self.model.layers:\n",
    "            if ll.name not in self.model.params:\n",
    "                continue\n",
    "            pp = self.model.params[ll.name]\n",
    "            gg = grad[ll.name]\n",
    "            for kk in pp.keys():\n",
    "                pp[kk] = pp[kk] - self.lr * gg[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def cross_entropy(p, y):\n",
    "    return -jnp.take(jnp.log(p), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jrng.PRNGKey(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = Model(rng, [Linear(10,10), Tanh(), Linear(10,10), Softmax()], loss=cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = SGD(mymodel, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7819085\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-c462635361e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-186-d27ae3787d0c>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mpp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "target_labels = numpy.floor(numpy.random.rand(256)).astype('int')\n",
    "inputs = numpy.random.rand(256,10)\n",
    "\n",
    "for ii in range(1000):\n",
    "    loss, grad = mymodel.loss_grad(inputs, target_labels)\n",
    "    \n",
    "    if numpy.mod(ii, 100) == 0:\n",
    "        print(loss)\n",
    "    optim.step(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
